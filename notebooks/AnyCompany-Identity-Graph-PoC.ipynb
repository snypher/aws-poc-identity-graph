{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f6d6c8e",
   "metadata": {},
   "source": [
    "## Check Notebook configuration and Neptune cluster Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a938c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%graph_notebook_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecb3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%graph_notebook_config\n",
    "{\n",
    "  \"host\": \"neptune-cluster-poc-identity-graph.cluster-c4k0tumhelmt.us-east-2.neptune.amazonaws.com\",\n",
    "  \"port\": 8182,\n",
    "  \"auth_mode\": \"IAM\",\n",
    "  \"load_from_s3_arn\": \"\",\n",
    "  \"ssl\": true,\n",
    "  \"aws_region\": \"us-east-2\",\n",
    "  \"sparql\": {\n",
    "    \"path\": \"sparql\"\n",
    "  },\n",
    "  \"gremlin\": {\n",
    "    \"traversal_source\": \"g\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3e838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%graph_notebook_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a0051",
   "metadata": {},
   "outputs": [],
   "source": [
    "%status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52028707",
   "metadata": {},
   "source": [
    "## Creating the Identity Graph\n",
    "\n",
    "The below steps simulate the process of seeding the identity graph with exported data from source data stores.\n",
    "\n",
    "It shows an example of how data from relational data sources (e.g. first-party customer data and transactional databases) as well as first-party behavioral data (e.i. cookies, device IDs and clickstream session) will be made available on CSV files uploaded to an Amazon S3 bucket.\n",
    "\n",
    "Then, an AWS Glue Crawler will discover the schema and store it as metadata into a centralized Data Catalog. An AWS Glue  ETL Job will pull the data from Amazon S3 and prepares it for initial load. Finally, the initial data load uses the Neptune bulk loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ebc7c9",
   "metadata": {},
   "source": [
    "### 1. Download the PoC's artifacts from S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee3b073",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "AWS_ACCOUNT_ID=\"733157031621\"\n",
    "BUCKET_NAME=\"poc-identity-graph-${AWS_ACCOUNT_ID}\"\n",
    "cd /home/ec2-user/SageMaker\n",
    "aws s3 sync s3://$BUCKET_NAME/scripts/ utils/\n",
    "aws s3 sync s3://$BUCKET_NAME/images/ images/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9295aaa9",
   "metadata": {},
   "source": [
    "### 2. Generate raw data for source datasets\n",
    "On this step we are simulating the replication tasks scheduled through AWS Database Migration Service (DMS) to pull relevant data from Relational and Non-relational source systems into an S3 bucket. See architecture diagram below\n",
    "\n",
    "<img src=\"../images/data-ingestion-bash-load-01.png\" alt=\"data-ingestion-bash-load-01\" width=\"1024\"/>\n",
    "\n",
    "Mock data for the below source datasets will be generated\n",
    "\n",
    "* First-party database (e.g. CRM)\n",
    "* Transactional database (e.g. purchases)\n",
    "* Cookie database\n",
    "* Click Stream database\n",
    "\n",
    "Resulting CSV files with raw data per each dataset\n",
    "\n",
    "* first-party: `first_party_data_full.csv`\n",
    "* cookie: `cookie_data_full.csv`\n",
    "* clickstream: `clickstream_data_full.csv`\n",
    "* transactional: `transactional_data_full.csv`\n",
    "\n",
    "Execution time for this process will depends on the compute resources available in your compute environment. Below are some references captured by running the script in different EC2 instance types and sizes\n",
    "\n",
    "* A volume of 10K first-party records could take 3-4 minutes when using a t3.large EC2 instance\n",
    "* A volume of 20K first-party records could take 10-11 minutes when using a t3.large EC2 instance\n",
    "* A volume of 100K first-party records could take about 85 minutes when using a m6i.4xlarge EC2 instance\n",
    "\n",
    "Additional parameters that can be optionaly used when generating raw data for source datasets:\n",
    "\n",
    "```sh\n",
    "usage: create-source-datasets.py [-h] [--records RECORDS]\n",
    "                                 [--uniqueness UNIQUENESS]\n",
    "                                 [--incremental {0,1}] [--debug {0,1}]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --records RECORDS     Amount of mock data records to generate. Default value\n",
    "                        is 10000\n",
    "  --uniqueness UNIQUENESS\n",
    "                        Uniqueness percentage of mock data generated for IPv4\n",
    "                        addresses and Device IDs. Default value is 30\n",
    "  --incremental {0,1}   Switch filenames suffix from \"_data_full\" to\n",
    "                        \"_data_inc\". Suffix is used for output CSV files\n",
    "                        indicating mock data generated will be used for\n",
    "                        initial load or incremental loads into the graph.\n",
    "                        Default value is 0\n",
    "  --debug {0,1}         Turn On/Off debugging (detailed output with muck data\n",
    "                        generated). Default value is 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6db7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install Faker >/dev/null 2>&1\n",
    "cd /home/ec2-user/SageMaker\n",
    "python3 utils/create-source-datasets.py --records 10000 --uniqueness 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d795600a",
   "metadata": {},
   "source": [
    "### 3. Visualizing raw data from source datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b585fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /home/ec2-user/SageMaker\n",
    "python3 utils/show-source-dataset-record.py \\\n",
    "    --csv-file /home/ec2-user/SageMaker/clickstream_data_full.csv \\\n",
    "    --dataset clickstream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c092428",
   "metadata": {},
   "source": [
    "### 4. Upload source datasets files to S3 bucket\n",
    "On this step we are simulating the replication tasks scheduled through AWS Database Migration Service (DMS) to store data pulled from Relational and Non-relational source systems into an S3 bucket. See architecture diagram below\n",
    "\n",
    "<img src=\"../images/data-ingestion-bash-load-02.png\" alt=\"data-ingestion-bash-load-02\" width=\"1024\"/>\n",
    "\n",
    "CSV files with raw data for initial load into the graph will be stored into the `datasets/sources/initial` S3 prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1021d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "AWS_ACCOUNT_ID=\"733157031621\"\n",
    "BUCKET_NAME=\"poc-identity-graph-${AWS_ACCOUNT_ID}\"\n",
    "PREFIX=\"datasets/sources/initial\"\n",
    "cd /home/ec2-user/SageMaker\n",
    "aws s3 cp first_party_data_full.csv s3://$BUCKET_NAME/$PREFIX/first_party/\n",
    "aws s3 cp cookie_data_full.csv s3://$BUCKET_NAME/$PREFIX/cookie/\n",
    "aws s3 cp clickstream_data_full.csv s3://$BUCKET_NAME/$PREFIX/clickstream/\n",
    "aws s3 cp transactional_data_full.csv s3://$BUCKET_NAME/$PREFIX/transactional/\n",
    "echo \"\"\n",
    "aws s3 ls s3://$BUCKET_NAME/$PREFIX/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c82059e",
   "metadata": {},
   "source": [
    "#### 4.1 Optionally, use the pre-generated datasets available on the S3 bucket (100K first-party records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca67e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "AWS_ACCOUNT_ID=\"733157031621\"\n",
    "BUCKET_NAME=\"poc-identity-graph-${AWS_ACCOUNT_ID}\"\n",
    "PREFIX_1=\"datasets/100K_sample\"\n",
    "PREFIX_2=\"datasets/sources/initial\"\n",
    "aws s3 sync s3://$BUCKET_NAME/$PREFIX_1/ s3://$BUCKET_NAME/$PREFIX_2/\n",
    "echo \"\"\n",
    "aws s3 ls s3://$BUCKET_NAME/$PREFIX_1/ --recursive\n",
    "echo \"\"\n",
    "aws s3 ls s3://$BUCKET_NAME/$PREFIX_2/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a4763b",
   "metadata": {},
   "source": [
    "### 5. Manually run the Glue Crawler `source-datasets-crawler-poc-identity-graph`\n",
    "\n",
    "The crawler `source-datasets-crawler-poc-identity-graph` is in charge for scanning raw CSV files stored in S3 path `s3://poc-identity-graph-733157031621/datasets/sources/initial/`, discover schema of these source datasets and stored it into the centralized Glue Data Catalog. A glue table definition is created per each source CSV file into the Glue database `database_poc_identity_graph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1aed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "REGION=\"us-east-2\"\n",
    "aws glue start-crawler \\\n",
    "--name source-datasets-crawler-poc-identity-graph \\\n",
    "--region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6357657c",
   "metadata": {},
   "source": [
    "### 6. Manually run the Glue Job `nodes-initial-load-s3-to-s3-poc-identity-graph`\n",
    "\n",
    "The Glue ETL job `nodes-initial-load-s3-to-s3-poc-identity-graph` create gremlin-formated CSV files related to Property Graph vertices (a.k.a. entities). It extracts data from Glue tables in Glue database `database_poc_identity_graph` (a.k.a. source datasets), apply simple transformations to schema and data like de-duplication and removes nulls, and generates gremlin-formated CSV files to ingest initial data into the identity graph using the Neptune bulk loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383acb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "REGION=\"us-east-2\"\n",
    "aws glue start-job-run \\\n",
    "--job-name nodes-initial-load-s3-to-s3-poc-identity-graph \\\n",
    "--region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fcffc9",
   "metadata": {},
   "source": [
    "### 7. Manually run the Glue Job `edges-initial-load-s3-to-s3-poc-identity-graph`\n",
    "\n",
    "The Glue ETL job `edges-initial-load-s3-to-s3-poc-identity-graph` create gremlin-formated CSV files related to Property Graph edges (a.k.a. relationships). It extracts data from Glue tables in Glue database `database_poc_identity_graph` (a.k.a. source datasets), apply simple transformations to schema and data like de-duplication and removes nulls, and generates gremlin-formated CSV files to ingest initial data into the identity graph using the Neptune bulk loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4448bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "REGION=\"us-east-2\"\n",
    "aws glue start-job-run \\\n",
    "--job-name edges-initial-load-s3-to-s3-poc-identity-graph \\\n",
    "--region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cb9683",
   "metadata": {},
   "source": [
    "### 8. If necessary, remove existent data in the identity graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b089cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%db_reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30e5c8f",
   "metadata": {},
   "source": [
    "### 9. Load initial data into the Identity Graph\n",
    "\n",
    "On steps 5, 6 and 7 we executing the AWS Glue steps needed to discover schema of source datasets as well as processing and prepare these datasets for the initial data load. See architecture diagram below\n",
    "\n",
    "<img src=\"../images/data-ingestion-bash-load-03.png\" alt=\"data-ingestion-bash-load-03\" width=\"1024\"/>\n",
    "\n",
    "Now let's initiate a bulk load using the Neptune bulk loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd52c8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d40e4",
   "metadata": {},
   "source": [
    "#### Get IDs of bulk load jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0767dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0ff162",
   "metadata": {},
   "source": [
    "#### Get the status of a provided bulk load job ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531ae4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_status 319ff0de-27be-4ec5-9688-2ccba18f3ecd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b0105",
   "metadata": {},
   "source": [
    "## Exploring the Identity Graph\n",
    "In order to better understand the data model and schema for the graph database we can execute the below graph queries to identify common entities like nodes/vertices and relationships/edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5891958",
   "metadata": {},
   "source": [
    "### Amount of nodes/vertices group by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e71f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "\n",
    "g.V().groupCount().by(label).unfold()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c6de33",
   "metadata": {},
   "source": [
    "### Amount of relationships/edges group by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ee8e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "\n",
    "g.E().groupCount().by(label).unfold()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae723818",
   "metadata": {},
   "source": [
    "### Top 10 Device IDs by amount of known users linked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00995b37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "\n",
    "g.V().hasLabel('DeviceID').\n",
    "    project('device_id','known_users_linked').\n",
    "        by(id).\n",
    "        by(in('usedDevice').out('loggedAs').count()).\n",
    "    order().\n",
    "        by('known_users_linked',desc).\n",
    "    limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0243fbba",
   "metadata": {},
   "source": [
    "### Top 10 Client IPs by amount of known users linked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88bcbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "\n",
    "g.V().hasLabel('ClientIP').\n",
    "    project('client_ip','known_users_linked').\n",
    "        by(id).\n",
    "        by(in('lastSeenAtIP').out('loggedAs').count()).\n",
    "    order().\n",
    "        by('known_users_linked',desc).\n",
    "    limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a88b07",
   "metadata": {},
   "source": [
    "### Top 10 Client IPs by amount of anonymous session linked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61fe39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "\n",
    "g.V().hasLabel('ClientIP').\n",
    "    project('client_ip','anonymous_sessions_linked','known_users_linked').\n",
    "        by(id).\n",
    "        by(in('lastSeenAtIP').not(out('loggedAs')).count()).\n",
    "        by(in('lastSeenAtIP').out('loggedAs').count()).\n",
    "    order().\n",
    "        by('anonymous_sessions_linked',desc).\n",
    "    limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acced06a",
   "metadata": {},
   "source": [
    "### Anonymous sessions related to a given Client IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee9f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin -p inv,outv\n",
    "\n",
    "g.V('198.43.127.104').\n",
    "    in('lastSeenAtIP').\n",
    "    not(out('loggedAs')).\n",
    "    path().\n",
    "    by()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca7b192",
   "metadata": {},
   "source": [
    "### Top 10 Client's External ID by amount of product purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de66cf3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "\n",
    "g.V().hasLabel('CustomerID').\n",
    "    project('external_id','amount_of_purchases').\n",
    "        by(id).\n",
    "        by(in('hasExternalId').out().out('hasPurchased').count()).\n",
    "    order().\n",
    "        by('amount_of_purchases',desc).\n",
    "    limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c0a445",
   "metadata": {},
   "source": [
    "## Identity Resolution Use Cases\n",
    "\n",
    "These use cases are part of online data consumption processes to deploy through a set of HTTP APIs to query the identity graph. The code logic to issue the below graph queries is implemented by different Lambda functions while the publishing and access to each API is managed by Amazon API Gateway. See architecture diagram below\n",
    "\n",
    "<img src=\"../images/data-consumption-online-query-02.png\" alt=\"data-consumption-online-query-02\" width=\"1024\"/>\n",
    "\n",
    "### Find out information about user interests based on the activity of the user across all devices\n",
    "Suppose you are hosting a web platform and collecting clickstream data as users browse your site or use your mobile app. In the majority of situations, users using your platform will be anonymous (or non-registered or logged in users). However, these anonymous users may be linked to other known users in that have used our platform before. We can join (or resolve) the identity of the anonymous user with attributes we know about existing users to make some assumptions (based off of known user behavior and heuristics) in order to know more about this anonymous user. We can then use this information to target the user with advertising, special offers, discounts, etc\n",
    "\n",
    "Let's use an example where we have an anonymous user interaction through AnyCompany Marketplace website (e.i. web session). The user's interaction is registered as session id **'bf4f32b7-41a5-49f9-898d-a494d376364a'**. We want to know more about this user and if it is linked to other users on our platform. This anonymous user is considered a **\"transient ID\"** in our graph data model. Assuming this user does not have a link to a known user, or **\"persistent ID\"**, how might we find connections from this transient ID to other known user IDs?\n",
    "\n",
    "Looking at the data model, you can see that **\"SessionID\"** (a.k.a. transient ID) vertices in our graph are connected to **\"ClientIP\"** vertices by an outgoing edge. We can traverse across \"ClientIP\" vertices to get to other linked \"SessionID\" that might be linked to a known user (a.k.a. persistent ID). Let's do that in the following graph query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8404ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin -p outv,inv,outv,inv\n",
    "\n",
    "g.V('bf4f32b7-41a5-49f9-898d-a494d376364a').\n",
    "    out('lastSeenAtIP').\n",
    "    in('lastSeenAtIP').\n",
    "    out('loggedAs').\n",
    "    dedup().\n",
    "    path()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28555116",
   "metadata": {},
   "source": [
    "Now we can dive deeper into the subgraph that describe the online interactions for one of the identified known users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5114f40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%gremlin -p inv,outv\n",
    "\n",
    "g.V('rivasanthony').\n",
    "    both('loggedAs','hasEmail','hasExternalId').\n",
    "    out().\n",
    "    simplePath().\n",
    "    path()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78a9f95",
   "metadata": {},
   "source": [
    "Let's use another example where we have an anonymous user interaction through AnyCompany's Mobile Application. The user's interaction is coming from a device with id **'EB7D73F94443EDC8'**. We want to know more about this user and if it is linked to other users on our platform. This anonymous user is considered a \"transient ID\" in our graph data model. Assuming this user does not have a link to a known user, or \"persistent ID\", how might we find connections from this transient ID to other known user IDs?\n",
    "\n",
    "Looking at the data model, you can see that **\"DeviceID\"** vertices in our graph are connected to **\"SessionID\"** (a.k.a. transient ID) vertices by an incomming edge. We can traverse across \"SessionID\" vertices that might be linked to a known user (a.k.a. persistent ID) and are connected to same \"DeviceID\" (EB7D73F94443EDC8). Let's do that in the following graph query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d59bb27",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%gremlin -p inv,outv,inv,inv,inv\n",
    "\n",
    "g.V('EB7D73F94443EDC8').\n",
    "    in('usedDevice').\n",
    "    out('loggedAs').\n",
    "    order().\n",
    "        by(id).\n",
    "    dedup().\n",
    "    out('hasEmail','hasExternalId').\n",
    "    out('hasPurchased').\n",
    "    path()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06a315",
   "metadata": {},
   "source": [
    "Let's use another example where we have an user interaction at the register of an AnyCompany's physical store. The user's interaction is registered with the customer external ID **'814-57-5109'** provided by the user at the register. We want to know more about this user and its purchase history, even if the user is linked to other known users on our platform.\n",
    "\n",
    "Looking at the data model, you can see that **\"ExternalID\"** vertices in our graph are connected to **\"User\"** (a.k.a. persistent ID) vertices by an incomming edge. We can traverse across \"User\" vertices to get other linked purchase identifiers (e.g. Email) that might have been used by same user when purchasing other products. Let's do that in the following graph query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e55ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin -p inv,outv,inv\n",
    "\n",
    "g.V('814-57-5109').\n",
    "    in('hasExternalId').\n",
    "    out().\n",
    "    out('hasPurchased').\n",
    "    path()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227606b5",
   "metadata": {},
   "source": [
    "## Adding new data into the Identity Graph\n",
    "\n",
    "The below steps simulate the process of ingesting updates from source data stores into the graph incrementally. It shows an example of how new data from relational data sources (first-party customer data and transactional databases) as well as first-party behavioral data (e.i. cookies, device IDs and clickstream session) can be added into the identity graph.\n",
    "\n",
    "This example showcase how new data should be added either by the bash data load process (using an AWS Glue ETL job) or by the online data load (using Amazon Lambda)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdb20e7",
   "metadata": {},
   "source": [
    "### 1. Generate raw data simulating incremental updates from source datasets\n",
    "\n",
    "Mock data for the below source datasets will be generated\n",
    "\n",
    "* First-party database (e.g. CRM)\n",
    "* Transactional database (e.g. purchases)\n",
    "* Cookie database\n",
    "* Click Stream database\n",
    "\n",
    "Resulting CSV files with raw data per each dataset\n",
    "\n",
    "* first-party: `first_party_data_inc.csv`\n",
    "* cookie: `cookie_data_inc.csv`\n",
    "* clickstream: `clickstream_data_inc.csv`\n",
    "* transactional: `transactional_data_inc.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc154212",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install Faker >/dev/null 2>&1\n",
    "cd /home/ec2-user/SageMaker\n",
    "python3 utils/create-source-datasets.py --records 100 --uniqueness 50 --incremental 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e175a662",
   "metadata": {},
   "source": [
    "### 2. Visualizing raw data from clickstream dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657290d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /home/ec2-user/SageMaker\n",
    "python3 utils/show-graph-entities-sample.py \\\n",
    "    --dataset clickstream \\\n",
    "    --csv-file clickstream_data_inc.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a5d277",
   "metadata": {},
   "source": [
    "### 3. Create property graph vertices and edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bfeedb",
   "metadata": {},
   "source": [
    "On this step we are simulating the online data load process to deploy through a set of HTTP APIs to add new data (incremental updates) to the identity graph. The code logic to issue the below graph queries is implemented by different Lambda functions while the publishing and access to each API is managed by Amazon API Gateway. See architecture diagram below\n",
    "\n",
    "<img src=\"../images/data-ingestion-online-load-02.png\" alt=\"data-ingestion-online-load-02\" width=\"1024\"/>\n",
    "\n",
    "The below graph query add new vertices and edges using in a single conditionally operation. The query uses input data from a single clickstream session (a.k.a. transient ID) to create the following entities used in our data model:\n",
    "\n",
    "* Vertex: _SessionID_\n",
    "* Vertex: _DomainName_\n",
    "* Vertex: _ClientIP_\n",
    "* Vertex: _DeviceID (in case any)_\n",
    "* Vertex: _User (in case any)_\n",
    "* Edge: _lastSeenAtDomain_ (from _SessionID_ to _DomainName_)\n",
    "* Edge: _lastSeenAtIP_ (from _SessionID_ to _ClientIP)\n",
    "* Edge: _usedDevice_ (from _SessionID_ to _DeviceID_)\n",
    "* Edge: _loggedAs_ (from _SessionID_ to _User_)\n",
    "\n",
    "New vertices and edges are only created if they don't exists previously in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e6354",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "\n",
    "g.V('202.211.203.86').\n",
    "    fold().\n",
    "    coalesce(unfold(), addV('ClientIP').property(id, '202.211.203.86')).\n",
    "    out('lastSeenAtIP').\n",
    "    fold().\n",
    "    coalesce(\n",
    "        unfold().hasId('b5e3f34b-7f42-4d8b-a14d-a54e209363ad'),\n",
    "        V('b5e3f34b-7f42-4d8b-a14d-a54e209363ad').\n",
    "        fold().\n",
    "        coalesce(\n",
    "            unfold(),\n",
    "            addV('SessionID').\n",
    "                property(id, 'b5e3f34b-7f42-4d8b-a14d-a54e209363ad').\n",
    "                property('client_platform','web').\n",
    "                property('canonical_url','http://mitchell.com/').\n",
    "                property('app_id','travel').\n",
    "                property('events',17).\n",
    "                property('start_timestamp','2021-02-12 09:48:36').\n",
    "                property('start_event','Purchase').\n",
    "                property('end_timestamp','2021-02-12 10:08:20').\n",
    "                property('end_event','Purchase').\n",
    "                property('session_duration_sec',1184)).\n",
    "            sideEffect(addE('lastSeenAtIP').to(V('202.211.203.86')))).\n",
    "    V('stewart.com').\n",
    "    fold().\n",
    "    coalesce(unfold(), addV('DomainName').property(id, 'stewart.com')).\n",
    "    out('lastSeenAtDomain').\n",
    "    fold().\n",
    "    coalesce(\n",
    "        unfold().hasId('b5e3f34b-7f42-4d8b-a14d-a54e209363ad'),\n",
    "        V('b5e3f34b-7f42-4d8b-a14d-a54e209363ad').\n",
    "            addE('lastSeenAtDomain').to(V('stewart.com'))).\n",
    "    V('EB7D73F94443EDC8').\n",
    "    fold().\n",
    "    coalesce(unfold(), addV('DeviceID').property(id, 'EB7D73F94443EDC8')).\n",
    "    out('usedDevice').\n",
    "    fold().\n",
    "    coalesce(\n",
    "        unfold().hasId('b5e3f34b-7f42-4d8b-a14d-a54e209363ad'),\n",
    "        V('b5e3f34b-7f42-4d8b-a14d-a54e209363ad').\n",
    "            addE('usedDevice').to(V('EB7D73F94443EDC8'))).\n",
    "    V('rivasanthony').\n",
    "    fold().\n",
    "    coalesce(unfold(), addV('User').property(id, 'rivasanthony')).\n",
    "    out('loggedAs').\n",
    "    fold().\n",
    "    coalesce(\n",
    "        unfold().hasId('b5e3f34b-7f42-4d8b-a14d-a54e209363ad'),\n",
    "        V('b5e3f34b-7f42-4d8b-a14d-a54e209363ad').\n",
    "            addE('loggedAs').to(V('rivasanthony'))).\n",
    "    iterate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7c2533",
   "metadata": {},
   "source": [
    "Once the new clickstream session (a.k.a. transient ID) is stored into the graph, lets use the below query to identify possible entities/vertices shared between the new session and existent sessions in the graph. Depending on the results, we could investigate further the resulting sessions in order to resolve identities of possible known users related (a.k.a. persistent IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e7ac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin -p outv,inv,outv,inv\n",
    "\n",
    "g.V('b5e3f34b-7f42-4d8b-a14d-a54e209363ad').\n",
    "    out().\n",
    "    in('lastSeenAtIP','lastSeenAtDomain','usedDevice','loggedAs').\n",
    "    dedup().\n",
    "    path()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b80a432",
   "metadata": {},
   "source": [
    "### 4. Upload source datasets files simulating incremental updates to S3 bucket (bash load)\n",
    "\n",
    "CSV files with raw data for incremental loads into the graph will be stored into the `datasets/sources/incremental/<DATASET_NAME>/${TIMESTAMP}` S3 prefix. Timestamp is calculated in UTC using the format `YYYYMMDD_HHmmss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd2d307",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "AWS_ACCOUNT_ID=\"733157031621\"\n",
    "BUCKET_NAME=\"poc-identity-graph-${AWS_ACCOUNT_ID}\"\n",
    "TIMESTAMP=$(date +%Y%m%d_%H%M%S)\n",
    "PREFIX=\"datasets/sources/incremental\"\n",
    "cd /home/ec2-user/SageMaker\n",
    "aws s3 cp first_party_data_inc.csv s3://$BUCKET_NAME/$PREFIX/first_party/${TIMESTAMP}/\n",
    "aws s3 cp cookie_data_inc.csv s3://$BUCKET_NAME/$PREFIX/cookie/${TIMESTAMP}/\n",
    "aws s3 cp clickstream_data_inc.csv s3://$BUCKET_NAME/$PREFIX/clickstream/${TIMESTAMP}/\n",
    "aws s3 cp transactional_data_inc.csv s3://$BUCKET_NAME/$PREFIX/transactional/${TIMESTAMP}/\n",
    "echo \"\"\n",
    "aws s3 ls s3://$BUCKET_NAME/$PREFIX/ --recursive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
